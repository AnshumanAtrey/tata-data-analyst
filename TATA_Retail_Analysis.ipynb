{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATA Online Retail Store: Revenue Drivers Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Business Studies with Applied AI | Group 11\n",
    "\n",
    "| # | Member |\n",
    "|---|--------|\n",
    "| 1 | **Payal Kunwar** |\n",
    "| 2 | **Gaurav Kulkarni** |\n",
    "| 3 | **Anshuman Atrey** |\n",
    "| 4 | **Abdullah Haque** |\n",
    "| 5 | **Shlok Vijay Kadam** |\n",
    "\n",
    "---\n",
    "\n",
    "## Business Problem Statement\n",
    "\n",
    "TATA's online retail division generates revenue from customers across **multiple countries**, selling a wide variety of products. However, leadership lacks a **data-driven understanding** of the key factors that drive revenue growth and customer retention. Without this insight, strategic decisions around marketing spend, inventory allocation, geographic expansion, and customer engagement remain sub-optimal.\n",
    "\n",
    "## Objective\n",
    "\n",
    "This project applies **Artificial Intelligence and Machine Learning techniques** to the TATA Online Retail transaction dataset to:\n",
    "\n",
    "1. **Identify and quantify** the primary drivers of revenue (products, countries, time patterns, customer segments).\n",
    "2. **Segment customers** using RFM analysis and K-Means clustering to enable targeted marketing strategies.\n",
    "3. **Predict high-value customers** using Logistic Regression to support proactive retention efforts.\n",
    "4. **Model revenue trends** using Linear Regression to support demand forecasting.\n",
    "5. **Translate analytical findings** into actionable business recommendations grounded in economic theory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We begin by importing all necessary Python libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Static visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Interactive visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('All libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "The dataset is the **UCI Online Retail Dataset**, containing transactional data for a UK-based online retail company between **December 2010 and December 2011**. Each row represents a line item from an invoice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. LOAD DATASET\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv('dataset/Online Retail Data Set.csv', encoding='latin1')\n",
    "\n",
    "print(f'Dataset Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "print(f'Memory Usage: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB')\n",
    "print('='*60)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info and structure\n",
    "print('DATASET INFO')\n",
    "print('='*60)\n",
    "df.info()\n",
    "print('\\n')\n",
    "print('DESCRIPTIVE STATISTICS')\n",
    "print('='*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print('MISSING VALUES SUMMARY')\n",
    "print('='*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "print(f'\\nTotal rows with null CustomerID: {df[\"CustomerID\"].isnull().sum():,}')\n",
    "print(f'Cancellation invoices (starting with C): {df[\"InvoiceNo\"].astype(str).str.startswith(\"C\").sum():,}')\n",
    "print(f'Negative Quantity rows: {(df[\"Quantity\"] < 0).sum():,}')\n",
    "print(f'Negative/Zero UnitPrice rows: {(df[\"UnitPrice\"] <= 0).sum():,}')\n",
    "print(f'Unique Countries: {df[\"Country\"].nunique()}')\n",
    "print(f'Unique Customers: {df[\"CustomerID\"].nunique()}')\n",
    "print(f'Unique Products: {df[\"StockCode\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Preprocessing\n",
    "\n",
    "Raw transactional data contains noise that can distort our analysis. We apply the following cleaning steps:\n",
    "\n",
    "| Step | Action | Rationale |\n",
    "|------|--------|-----------|\n",
    "| 1 | Remove cancelled orders (InvoiceNo starts with 'C') | Cancellations do not represent actual revenue |\n",
    "| 2 | Remove rows with Quantity <= 0 | Negative/zero quantities are returns or data errors |\n",
    "| 3 | Remove rows with UnitPrice <= 0 | Zero or negative prices are likely adjustments, not real sales |\n",
    "| 4 | Drop null CustomerID rows | Required for customer-level analysis (RFM, clustering) |\n",
    "| 5 | Create TotalRevenue column | Revenue = Quantity x UnitPrice |\n",
    "| 6 | Parse dates and extract time features | Enable temporal trend analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. DATA CLEANING & PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "shape_before = df.shape\n",
    "print(f'Shape BEFORE cleaning: {shape_before[0]:,} rows x {shape_before[1]} columns')\n",
    "print('-'*60)\n",
    "\n",
    "# Step 1: Remove cancelled orders (InvoiceNo starting with 'C')\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
    "cancelled = df['InvoiceNo'].str.startswith('C').sum()\n",
    "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
    "print(f'Step 1 - Removed {cancelled:,} cancelled orders')\n",
    "\n",
    "# Step 2: Remove rows with Quantity <= 0\n",
    "neg_qty = (df['Quantity'] <= 0).sum()\n",
    "df = df[df['Quantity'] > 0]\n",
    "print(f'Step 2 - Removed {neg_qty:,} rows with Quantity <= 0')\n",
    "\n",
    "# Step 3: Remove rows with UnitPrice <= 0\n",
    "neg_price = (df['UnitPrice'] <= 0).sum()\n",
    "df = df[df['UnitPrice'] > 0]\n",
    "print(f'Step 3 - Removed {neg_price:,} rows with UnitPrice <= 0')\n",
    "\n",
    "# Step 4: Drop rows with null CustomerID\n",
    "null_cust = df['CustomerID'].isnull().sum()\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "df['CustomerID'] = df['CustomerID'].astype(int)\n",
    "print(f'Step 4 - Removed {null_cust:,} rows with null CustomerID')\n",
    "\n",
    "# Step 5: Create TotalRevenue column\n",
    "df['TotalRevenue'] = df['Quantity'] * df['UnitPrice']\n",
    "print(f'Step 5 - Created TotalRevenue column')\n",
    "\n",
    "# Step 6: Parse InvoiceDate and extract time features\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='mixed', dayfirst=True)\n",
    "df['Year'] = df['InvoiceDate'].dt.year\n",
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['DayName'] = df['InvoiceDate'].dt.day_name()\n",
    "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
    "print(f'Step 6 - Parsed dates and extracted Month, Year, Hour, DayOfWeek')\n",
    "\n",
    "print('-'*60)\n",
    "shape_after = df.shape\n",
    "print(f'Shape AFTER cleaning: {shape_after[0]:,} rows x {shape_after[1]} columns')\n",
    "print(f'Rows removed: {shape_before[0] - shape_after[0]:,} ({(shape_before[0] - shape_after[0])/shape_before[0]*100:.1f}%)')\n",
    "print(f'\\nTotal Revenue in cleaned dataset: GBP {df[\"TotalRevenue\"].sum():,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics after cleaning\n",
    "print('CLEANED DATASET - SUMMARY STATISTICS')\n",
    "print('='*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the cleaned data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "With clean data in hand, we now explore the key dimensions of revenue:\n",
    "- **Temporal patterns** (monthly trends, hourly patterns, day-of-week effects)\n",
    "- **Geographic distribution** (revenue by country)\n",
    "- **Product analysis** (top-selling items)\n",
    "- **Customer behavior** (purchase frequency)\n",
    "- **Correlations** between numerical features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Monthly Revenue Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.1 MONTHLY REVENUE TREND\n",
    "# ============================================================\n",
    "\n",
    "monthly_revenue = df.groupby('YearMonth').agg(\n",
    "    Revenue=('TotalRevenue', 'sum'),\n",
    "    Orders=('InvoiceNo', 'nunique'),\n",
    "    Customers=('CustomerID', 'nunique')\n",
    ").reset_index()\n",
    "monthly_revenue['YearMonth'] = monthly_revenue['YearMonth'].astype(str)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Monthly Revenue (GBP)', 'Monthly Unique Orders & Customers'),\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# Revenue line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=monthly_revenue['YearMonth'],\n",
    "        y=monthly_revenue['Revenue'],\n",
    "        mode='lines+markers',\n",
    "        name='Revenue (GBP)',\n",
    "        line=dict(color='#2ecc71', width=3),\n",
    "        marker=dict(size=8),\n",
    "        fill='tozeroy',\n",
    "        fillcolor='rgba(46, 204, 113, 0.15)'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Orders and customers\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=monthly_revenue['YearMonth'],\n",
    "        y=monthly_revenue['Orders'],\n",
    "        mode='lines+markers',\n",
    "        name='Unique Orders',\n",
    "        line=dict(color='#3498db', width=2)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=monthly_revenue['YearMonth'],\n",
    "        y=monthly_revenue['Customers'],\n",
    "        mode='lines+markers',\n",
    "        name='Unique Customers',\n",
    "        line=dict(color='#e74c3c', width=2)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    title_text='Revenue & Customer Activity Over Time',\n",
    "    title_font_size=20,\n",
    "    template='plotly_dark',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_yaxes(title_text='Revenue (GBP)', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Count', row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Top 10 Revenue-Generating Countries (Excluding UK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.2 TOP 10 REVENUE-GENERATING COUNTRIES (Excluding UK)\n",
    "# ============================================================\n",
    "\n",
    "country_revenue = df.groupby('Country')['TotalRevenue'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Show UK dominance first\n",
    "uk_rev = country_revenue[country_revenue['Country'] == 'United Kingdom']['TotalRevenue'].values[0]\n",
    "total_rev = country_revenue['TotalRevenue'].sum()\n",
    "print(f'UK Revenue: GBP {uk_rev:,.2f} ({uk_rev/total_rev*100:.1f}% of total)')\n",
    "print(f'Non-UK Revenue: GBP {total_rev - uk_rev:,.2f} ({(total_rev-uk_rev)/total_rev*100:.1f}% of total)')\n",
    "\n",
    "# Plot top 10 excluding UK for better visibility\n",
    "top10_no_uk = country_revenue[country_revenue['Country'] != 'United Kingdom'].head(10)\n",
    "\n",
    "fig = px.bar(\n",
    "    top10_no_uk,\n",
    "    x='TotalRevenue',\n",
    "    y='Country',\n",
    "    orientation='h',\n",
    "    color='TotalRevenue',\n",
    "    color_continuous_scale='Viridis',\n",
    "    title='Top 10 Revenue-Generating Countries (Excluding UK)',\n",
    "    labels={'TotalRevenue': 'Total Revenue (GBP)', 'Country': ''},\n",
    "    text=top10_no_uk['TotalRevenue'].apply(lambda x: f'GBP {x:,.0f}')\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    title_font_size=18,\n",
    "    yaxis=dict(autorange='reversed')\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Top 20 Best-Selling Products by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.3 TOP 20 BEST-SELLING PRODUCTS BY REVENUE\n",
    "# ============================================================\n",
    "\n",
    "product_revenue = df.groupby(['StockCode', 'Description']).agg(\n",
    "    Revenue=('TotalRevenue', 'sum'),\n",
    "    Quantity=('Quantity', 'sum'),\n",
    "    Orders=('InvoiceNo', 'nunique')\n",
    ").sort_values('Revenue', ascending=False).head(20).reset_index()\n",
    "\n",
    "# Truncate long descriptions for display\n",
    "product_revenue['ShortDesc'] = product_revenue['Description'].str[:40]\n",
    "\n",
    "fig = px.bar(\n",
    "    product_revenue,\n",
    "    x='Revenue',\n",
    "    y='ShortDesc',\n",
    "    orientation='h',\n",
    "    color='Revenue',\n",
    "    color_continuous_scale='Turbo',\n",
    "    title='Top 20 Best-Selling Products by Revenue',\n",
    "    labels={'Revenue': 'Total Revenue (GBP)', 'ShortDesc': 'Product'},\n",
    "    hover_data={'Description': True, 'Quantity': True, 'Orders': True}\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=700,\n",
    "    title_font_size=18,\n",
    "    yaxis=dict(autorange='reversed')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Revenue Distribution by Day of Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.4 REVENUE DISTRIBUTION BY DAY OF WEEK\n",
    "# ============================================================\n",
    "\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_revenue = df.groupby('DayName').agg(\n",
    "    Revenue=('TotalRevenue', 'sum'),\n",
    "    AvgOrderValue=('TotalRevenue', 'mean'),\n",
    "    Orders=('InvoiceNo', 'nunique')\n",
    ").reindex(day_order).reset_index()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Total Revenue by Day of Week', 'Average Order Value by Day of Week')\n",
    ")\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c', '#e67e22']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=daily_revenue['DayName'],\n",
    "        y=daily_revenue['Revenue'],\n",
    "        marker_color=colors,\n",
    "        name='Total Revenue',\n",
    "        text=daily_revenue['Revenue'].apply(lambda x: f'GBP {x:,.0f}'),\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=daily_revenue['DayName'],\n",
    "        y=daily_revenue['AvgOrderValue'],\n",
    "        marker_color=colors,\n",
    "        name='Avg Order Value',\n",
    "        text=daily_revenue['AvgOrderValue'].apply(lambda x: f'GBP {x:.2f}'),\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    title_text='Revenue Patterns by Day of Week',\n",
    "    title_font_size=18,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hourly Sales Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.5 HOURLY SALES PATTERN\n",
    "# ============================================================\n",
    "\n",
    "hourly_revenue = df.groupby('Hour').agg(\n",
    "    Revenue=('TotalRevenue', 'sum'),\n",
    "    Transactions=('InvoiceNo', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hourly_revenue['Hour'],\n",
    "        y=hourly_revenue['Revenue'],\n",
    "        mode='lines+markers',\n",
    "        name='Revenue (GBP)',\n",
    "        line=dict(color='#2ecc71', width=3),\n",
    "        marker=dict(size=8),\n",
    "        fill='tozeroy',\n",
    "        fillcolor='rgba(46, 204, 113, 0.1)'\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=hourly_revenue['Hour'],\n",
    "        y=hourly_revenue['Transactions'],\n",
    "        name='Unique Transactions',\n",
    "        marker_color='rgba(52, 152, 219, 0.5)',\n",
    "        opacity=0.6\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Hourly Sales Pattern: Revenue & Transaction Volume',\n",
    "    title_font_size=18,\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    xaxis_title='Hour of Day',\n",
    "    xaxis=dict(dtick=1)\n",
    ")\n",
    "fig.update_yaxes(title_text='Revenue (GBP)', secondary_y=False)\n",
    "fig.update_yaxes(title_text='Unique Transactions', secondary_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Revenue by Country - World Map (Choropleth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.6 REVENUE BY COUNTRY - WORLD MAP (CHOROPLETH)\n",
    "# ============================================================\n",
    "\n",
    "# Map country names to ISO codes for plotly choropleth\n",
    "country_iso = {\n",
    "    'United Kingdom': 'GBR', 'France': 'FRA', 'Australia': 'AUS', 'Netherlands': 'NLD',\n",
    "    'Germany': 'DEU', 'Norway': 'NOR', 'EIRE': 'IRL', 'Switzerland': 'CHE',\n",
    "    'Spain': 'ESP', 'Poland': 'POL', 'Portugal': 'PRT', 'Italy': 'ITA',\n",
    "    'Belgium': 'BEL', 'Lithuania': 'LTU', 'Japan': 'JPN', 'Iceland': 'ISL',\n",
    "    'Channel Islands': 'GBR', 'Denmark': 'DNK', 'Cyprus': 'CYP', 'Sweden': 'SWE',\n",
    "    'Austria': 'AUT', 'Israel': 'ISR', 'Finland': 'FIN', 'Bahrain': 'BHR',\n",
    "    'Greece': 'GRC', 'Hong Kong': 'HKG', 'Singapore': 'SGP', 'Lebanon': 'LBN',\n",
    "    'United Arab Emirates': 'ARE', 'Saudi Arabia': 'SAU', 'Czech Republic': 'CZE',\n",
    "    'Canada': 'CAN', 'Unspecified': None, 'Brazil': 'BRA', 'USA': 'USA',\n",
    "    'European Community': None, 'Malta': 'MLT', 'RSA': 'ZAF'\n",
    "}\n",
    "\n",
    "country_rev = df.groupby('Country')['TotalRevenue'].sum().reset_index()\n",
    "country_rev['ISO'] = country_rev['Country'].map(country_iso)\n",
    "country_rev = country_rev.dropna(subset=['ISO'])\n",
    "\n",
    "fig = px.choropleth(\n",
    "    country_rev,\n",
    "    locations='ISO',\n",
    "    color='TotalRevenue',\n",
    "    hover_name='Country',\n",
    "    color_continuous_scale='Plasma',\n",
    "    title='Global Revenue Distribution by Country',\n",
    "    labels={'TotalRevenue': 'Revenue (GBP)'},\n",
    "    projection='natural earth'\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=600,\n",
    "    title_font_size=18,\n",
    "    geo=dict(showframe=False, showcoastlines=True, coastlinecolor='gray')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Customer Purchase Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.7 CUSTOMER PURCHASE FREQUENCY DISTRIBUTION\n",
    "# ============================================================\n",
    "\n",
    "customer_freq = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "customer_freq.columns = ['CustomerID', 'PurchaseFrequency']\n",
    "\n",
    "fig = px.histogram(\n",
    "    customer_freq,\n",
    "    x='PurchaseFrequency',\n",
    "    nbins=50,\n",
    "    title='Customer Purchase Frequency Distribution',\n",
    "    labels={'PurchaseFrequency': 'Number of Purchases', 'count': 'Number of Customers'},\n",
    "    color_discrete_sequence=['#3498db']\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    title_font_size=18,\n",
    "    xaxis_title='Number of Purchases',\n",
    "    yaxis_title='Number of Customers'\n",
    ")\n",
    "\n",
    "# Add annotation for median\n",
    "median_freq = customer_freq['PurchaseFrequency'].median()\n",
    "fig.add_vline(x=median_freq, line_dash='dash', line_color='red',\n",
    "              annotation_text=f'Median: {median_freq:.0f}', annotation_position='top right')\n",
    "fig.show()\n",
    "\n",
    "print(f'Purchase Frequency Statistics:')\n",
    "print(f'  Mean:   {customer_freq[\"PurchaseFrequency\"].mean():.1f} orders/customer')\n",
    "print(f'  Median: {customer_freq[\"PurchaseFrequency\"].median():.0f} orders/customer')\n",
    "print(f'  Max:    {customer_freq[\"PurchaseFrequency\"].max()} orders/customer')\n",
    "print(f'  Customers with only 1 purchase: {(customer_freq[\"PurchaseFrequency\"]==1).sum():,} ({(customer_freq[\"PurchaseFrequency\"]==1).mean()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4.8 CORRELATION HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "numeric_cols = ['Quantity', 'UnitPrice', 'TotalRevenue', 'Month', 'Hour', 'DayOfWeek']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto='.2f',\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    title='Correlation Heatmap of Numerical Features',\n",
    "    labels=dict(color='Correlation'),\n",
    "    aspect='auto'\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=550,\n",
    "    title_font_size=18\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. RFM Analysis (Recency, Frequency, Monetary)\n",
    "\n",
    "RFM analysis is a **customer segmentation technique** based on three behavioral metrics:\n",
    "\n",
    "| Metric | Definition | Business Meaning |\n",
    "|--------|-----------|------------------|\n",
    "| **Recency (R)** | Days since last purchase | How recently did the customer buy? |\n",
    "| **Frequency (F)** | Number of unique purchases | How often does the customer buy? |\n",
    "| **Monetary (M)** | Total amount spent | How much does the customer spend? |\n",
    "\n",
    "Customers who bought **recently**, buy **frequently**, and spend **more** are the most valuable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. RFM ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Reference date = max date in dataset + 1 day\n",
    "reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "print(f'Reference date for Recency calculation: {reference_date}')\n",
    "\n",
    "# Calculate RFM per customer\n",
    "rfm = df.groupby('CustomerID').agg(\n",
    "    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),\n",
    "    Frequency=('InvoiceNo', 'nunique'),\n",
    "    Monetary=('TotalRevenue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "print(f'\\nRFM Table Shape: {rfm.shape}')\n",
    "print(f'Total unique customers analyzed: {rfm.shape[0]:,}')\n",
    "print('\\nRFM Summary Statistics:')\n",
    "rfm[['Recency', 'Frequency', 'Monetary']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Distribution plots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Recency (days)', 'Frequency (orders)', 'Monetary (GBP)')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm['Recency'], nbinsx=50, marker_color='#e74c3c', name='Recency', opacity=0.8),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm['Frequency'], nbinsx=50, marker_color='#3498db', name='Frequency', opacity=0.8),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=rfm['Monetary'], nbinsx=50, marker_color='#2ecc71', name='Monetary', opacity=0.8),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='RFM Distributions',\n",
    "    title_font_size=18,\n",
    "    template='plotly_dark',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.1 ASSIGN RFM SCORES (1-4 using quantiles)\n",
    "# ============================================================\n",
    "\n",
    "# For Recency: lower is better, so we invert the scoring\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], q=4, labels=[4, 3, 2, 1]).astype(int)\n",
    "\n",
    "# For Frequency and Monetary: higher is better\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=4, labels=[1, 2, 3, 4]).astype(int)\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=4, labels=[1, 2, 3, 4]).astype(int)\n",
    "\n",
    "# Combined RFM Score\n",
    "rfm['RFM_Score'] = rfm['R_Score'] + rfm['F_Score'] + rfm['M_Score']\n",
    "\n",
    "print('RFM Scores assigned (1-4 scale, 4 = best).')\n",
    "print(f'RFM_Score range: {rfm[\"RFM_Score\"].min()} to {rfm[\"RFM_Score\"].max()}')\n",
    "rfm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5.2 CREATE CUSTOMER SEGMENTS\n",
    "# ============================================================\n",
    "\n",
    "def assign_segment(row):\n",
    "    \"\"\"Assign customer segment based on RFM scores.\"\"\"\n",
    "    r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
    "\n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif r >= 3 and f >= 3 and m >= 3:\n",
    "        return 'Loyal Customers'\n",
    "    elif r >= 3 and f >= 2 and m >= 2:\n",
    "        return 'Potential Loyalists'\n",
    "    elif r >= 4 and f <= 2:\n",
    "        return 'New Customers'\n",
    "    elif r >= 2 and f >= 2 and m >= 2:\n",
    "        return 'Promising'\n",
    "    elif r <= 2 and f >= 3 and m >= 3:\n",
    "        return 'At Risk'\n",
    "    elif r <= 2 and f >= 4 and m >= 4:\n",
    "        return 'Cannot Lose Them'\n",
    "    elif r <= 2 and f <= 2 and m <= 2:\n",
    "        return 'Lost'\n",
    "    elif r <= 2 and f <= 2:\n",
    "        return 'Hibernating'\n",
    "    else:\n",
    "        return 'Need Attention'\n",
    "\n",
    "rfm['Segment'] = rfm.apply(assign_segment, axis=1)\n",
    "\n",
    "# Segment summary\n",
    "segment_summary = rfm.groupby('Segment').agg(\n",
    "    Count=('CustomerID', 'count'),\n",
    "    Avg_Recency=('Recency', 'mean'),\n",
    "    Avg_Frequency=('Frequency', 'mean'),\n",
    "    Avg_Monetary=('Monetary', 'mean'),\n",
    "    Total_Revenue=('Monetary', 'sum')\n",
    ").sort_values('Total_Revenue', ascending=False).round(1)\n",
    "\n",
    "segment_summary['Revenue_Pct'] = (segment_summary['Total_Revenue'] / segment_summary['Total_Revenue'].sum() * 100).round(1)\n",
    "print('CUSTOMER SEGMENT SUMMARY')\n",
    "print('='*80)\n",
    "segment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Visualization - Pie Chart & Bar Chart\n",
    "segment_counts = rfm['Segment'].value_counts().reset_index()\n",
    "segment_counts.columns = ['Segment', 'Count']\n",
    "\n",
    "segment_rev = rfm.groupby('Segment')['Monetary'].sum().sort_values(ascending=True).reset_index()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n",
    "    subplot_titles=('Customer Distribution by Segment', 'Total Revenue by Segment')\n",
    ")\n",
    "\n",
    "colors_seg = px.colors.qualitative.Set2\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=segment_counts['Segment'],\n",
    "        values=segment_counts['Count'],\n",
    "        hole=0.4,\n",
    "        marker_colors=colors_seg,\n",
    "        textinfo='percent+label',\n",
    "        textfont_size=10\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=segment_rev['Monetary'],\n",
    "        y=segment_rev['Segment'],\n",
    "        orientation='h',\n",
    "        marker_color=colors_seg[:len(segment_rev)],\n",
    "        text=segment_rev['Monetary'].apply(lambda x: f'GBP {x:,.0f}'),\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='RFM Customer Segmentation Results',\n",
    "    title_font_size=20,\n",
    "    template='plotly_dark',\n",
    "    height=550,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. K-Means Clustering\n",
    "\n",
    "We apply **K-Means**, an unsupervised machine learning algorithm, to cluster customers based on their **RFM values**. Unlike the rule-based RFM segmentation above, K-Means finds natural groupings in the data without predefined boundaries.\n",
    "\n",
    "**Steps:**\n",
    "1. Standardize RFM values (zero mean, unit variance) to prevent scale bias.\n",
    "2. Use the **Elbow Method** and **Silhouette Scores** to determine the optimal number of clusters.\n",
    "3. Fit K-Means and analyze each cluster's business profile.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. K-MEANS CLUSTERING\n",
    "# ============================================================\n",
    "\n",
    "# Prepare features for clustering\n",
    "rfm_features = rfm[['Recency', 'Frequency', 'Monetary']].copy()\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_features)\n",
    "\n",
    "print('Features standardized using StandardScaler.')\n",
    "print(f'Scaled data shape: {rfm_scaled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6.1 ELBOW METHOD + SILHOUETTE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels_temp = kmeans_temp.fit_predict(rfm_scaled)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(rfm_scaled, labels_temp))\n",
    "    print(f'  k={k}: Inertia={kmeans_temp.inertia_:,.0f}, Silhouette={silhouette_scores[-1]:.4f}')\n",
    "\n",
    "# Plot Elbow and Silhouette side by side\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Elbow Method (Inertia vs K)', 'Silhouette Score vs K')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(K_range), y=inertias,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='#e74c3c', width=3),\n",
    "        marker=dict(size=10),\n",
    "        name='Inertia'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(K_range), y=silhouette_scores,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='#2ecc71', width=3),\n",
    "        marker=dict(size=10),\n",
    "        name='Silhouette'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Number of Clusters (K)', dtick=1, row=1, col=1)\n",
    "fig.update_xaxes(title_text='Number of Clusters (K)', dtick=1, row=1, col=2)\n",
    "fig.update_yaxes(title_text='Inertia', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Silhouette Score', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Determining Optimal Number of Clusters',\n",
    "    title_font_size=18,\n",
    "    template='plotly_dark',\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Select optimal k\n",
    "optimal_k = 4\n",
    "print(f'\\nOptimal K selected: {optimal_k} (based on elbow point and silhouette balance)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6.2 FIT K-MEANS WITH OPTIMAL K\n",
    "# ============================================================\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, max_iter=300)\n",
    "rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "print(f'K-Means fitted with K={optimal_k}')\n",
    "print(f'Final Inertia: {kmeans.inertia_:,.2f}')\n",
    "print(f'Silhouette Score: {silhouette_score(rfm_scaled, rfm[\"Cluster\"]):.4f}')\n",
    "\n",
    "# ── Re-map clusters so Cluster 0 = highest avg Monetary (matches utils/data.py logic) ──\n",
    "order = rfm.groupby('Cluster')['Monetary'].mean().sort_values(ascending=False).index.tolist()\n",
    "cluster_map = {old: new for new, old in enumerate(order)}\n",
    "rfm['Cluster'] = rfm['Cluster'].map(cluster_map)\n",
    "print(f'\\nCluster re-mapping (0 = highest value): {cluster_map}')\n",
    "\n",
    "# ── Assign human-readable names (aligned with Streamlit app config) ──\n",
    "CLUSTER_NAMES = {0: 'Champions', 1: 'Potential Loyalists', 2: 'At-Risk', 3: 'Hibernating'}\n",
    "CLUSTER_ACCENT = {0: '#16a34a', 1: '#2563eb', 2: '#d97706', 3: '#dc2626'}\n",
    "rfm['Segment_ML'] = rfm['Cluster'].map(CLUSTER_NAMES)\n",
    "\n",
    "print(f'\\nCluster Distribution (after re-mapping):')\n",
    "print(rfm['Cluster'].value_counts().sort_index())\n",
    "print(f'\\nSegment Distribution:')\n",
    "print(rfm['Segment_ML'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6.3 3D SCATTER PLOT OF CLUSTERS\n",
    "# ============================================================\n",
    "\n",
    "sample = rfm.sample(n=min(3000, len(rfm)), random_state=42)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    sample,\n",
    "    x='Recency',\n",
    "    y='Frequency',\n",
    "    z='Monetary',\n",
    "    color='Segment_ML',\n",
    "    color_discrete_map={v: CLUSTER_ACCENT[k] for k, v in CLUSTER_NAMES.items()},\n",
    "    title='K-Means Customer Clusters (3D View)',\n",
    "    labels={'Recency': 'Recency (days)', 'Frequency': 'Frequency (orders)', 'Monetary': 'Monetary (GBP)'},\n",
    "    opacity=0.65,\n",
    "    hover_data={'CustomerID': True, 'Recency': True, 'Frequency': True, 'Monetary': ':.2f'}\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    height=700,\n",
    "    title_font_size=18,\n",
    "    scene=dict(\n",
    "        xaxis_title='Recency (days)',\n",
    "        yaxis_title='Frequency (orders)',\n",
    "        zaxis_title='Monetary (GBP)'\n",
    "    ),\n",
    "    legend=dict(title='Segment', font=dict(size=11))\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6.4 CLUSTER PROFILE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "cluster_profile = rfm.groupby('Cluster').agg(\n",
    "    Segment=('Segment_ML', 'first'),\n",
    "    Count=('CustomerID', 'count'),\n",
    "    Avg_Recency=('Recency', 'mean'),\n",
    "    Avg_Frequency=('Frequency', 'mean'),\n",
    "    Avg_Monetary=('Monetary', 'mean'),\n",
    "    Total_Revenue=('Monetary', 'sum')\n",
    ").round(2)\n",
    "\n",
    "cluster_profile['Revenue_Share_%'] = (cluster_profile['Total_Revenue'] / cluster_profile['Total_Revenue'].sum() * 100).round(1)\n",
    "cluster_profile['Customer_Share_%'] = (cluster_profile['Count'] / cluster_profile['Count'].sum() * 100).round(1)\n",
    "\n",
    "print('CLUSTER PROFILES (aligned with Streamlit app)')\n",
    "print('='*90)\n",
    "cluster_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize cluster profiles with grouped bar chart\n",
    "cp = cluster_profile.reset_index()\n",
    "cp['Label'] = cp['Cluster'].astype(str) + ' – ' + cp['Segment']\n",
    "cluster_colors = [CLUSTER_ACCENT[i] for i in cp['Cluster']]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Avg Recency (days) ↓ better', 'Avg Frequency (orders) ↑ better', 'Avg Monetary (GBP) ↑ better')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=cp['Label'], y=cp['Avg_Recency'], marker_color=cluster_colors,\n",
    "           text=cp['Avg_Recency'].round(0), textposition='outside', name='Recency'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=cp['Label'], y=cp['Avg_Frequency'], marker_color=cluster_colors,\n",
    "           text=cp['Avg_Frequency'].round(1), textposition='outside', name='Frequency'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=cp['Label'], y=cp['Avg_Monetary'], marker_color=cluster_colors,\n",
    "           text=cp['Avg_Monetary'].apply(lambda x: f'{x:,.0f}'), textposition='outside', name='Monetary'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Cluster Profiles: Average RFM Values by Segment',\n",
    "    title_font_size=18,\n",
    "    template='plotly_white',\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_xaxes(tickangle=-20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Business Interpretation of K-Means Clusters\n",
    "\n",
    "Clusters are re-mapped so **Cluster 0 always has the highest average Monetary** (matches deployed app). The segment names are identical to those shown in the Streamlit dashboard.\n",
    "\n",
    "| Cluster | Segment Name | Recency | Frequency | Monetary | Business Strategy |\n",
    "|---------|--------------|---------|-----------|----------|-------------------|\n",
    "| **0** | **Champions** | Low (recent) | High | High | VIP loyalty rewards, early access to launches, referral incentives |\n",
    "| **1** | **Potential Loyalists** | Moderate | Moderate | Moderate | Upsell & cross-sell, tiered loyalty enrollment, free shipping |\n",
    "| **2** | **At-Risk** | High (lapsed) | Was High | Was High | Win-back emails, comeback offers, disengagement survey |\n",
    "| **3** | **Hibernating** | High (dormant) | Low | Low | Aggressive reactivation discount, low-cost trial offers |\n",
    "\n",
    "> **App alignment:** This mapping is generated by `utils/data.py → train_models()`, which sorts raw KMeans cluster IDs by descending mean Monetary, then assigns labels via `CLUSTER_NAMES` in `utils/config.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Logistic Regression: Predicting High-Value Customers\n",
    "\n",
    "We build a **supervised classification model** to predict whether a customer is **High-Value** (Monetary > median) or **Low-Value** (Monetary <= median) based on their Recency and Frequency.\n",
    "\n",
    "**Business Application:** This model can be used to identify which new or returning customers are likely to become high-value, enabling proactive marketing and retention strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7. LOGISTIC REGRESSION\n",
    "# ============================================================\n",
    "# NOTE: We use the same StandardScaler + same 3 features (Recency, Frequency, Monetary)\n",
    "# as KMeans — this is exactly how utils/data.py → train_models() works in the app.\n",
    "\n",
    "# Create binary target: High-Value = 1 if Monetary > median, else 0\n",
    "monetary_median = rfm['Monetary'].median()\n",
    "rfm['HighValue'] = (rfm['Monetary'] > monetary_median).astype(int)\n",
    "\n",
    "print(f'Monetary Median (threshold): GBP {monetary_median:,.2f}')\n",
    "print(f'High-Value customers: {rfm[\"HighValue\"].sum():,} ({rfm[\"HighValue\"].mean()*100:.1f}%)')\n",
    "print(f'Low-Value customers:  {(rfm[\"HighValue\"]==0).sum():,} ({(1-rfm[\"HighValue\"].mean())*100:.1f}%)')\n",
    "\n",
    "# ── Use the already-scaled RFM features from Section 6 (matches deployed app) ──\n",
    "# rfm_scaled contains scaled [Recency, Frequency, Monetary]\n",
    "X = rfm_scaled\n",
    "y = rfm['HighValue'].values\n",
    "\n",
    "# Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f'\\nFeatures used: Scaled [Recency, Frequency, Monetary] — identical to KMeans input')\n",
    "print(f'  (Aligns with utils/data.py → train_models() in the Streamlit app)')\n",
    "print(f'\\nTrain size: {len(X_train):,}')\n",
    "print(f'Test size:  {len(X_test):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classification Report\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Low-Value', 'High-Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.1 CONFUSION MATRIX HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Low-Value', 'High-Value'],\n",
    "    yticklabels=['Low-Value', 'High-Value'],\n",
    "    linewidths=1, linecolor='white',\n",
    "    annot_kws={'size': 16}\n",
    ")\n",
    "ax.set_xlabel('Predicted Label', fontsize=14)\n",
    "ax.set_ylabel('True Label', fontsize=14)\n",
    "ax.set_title('Confusion Matrix - Logistic Regression', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "accuracy = (cm[0,0] + cm[1,1]) / cm.sum()\n",
    "print(f'Accuracy: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7.2 ROC CURVE WITH AUC SCORE\n",
    "# ============================================================\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr, y=tpr,\n",
    "    mode='lines',\n",
    "    name=f'Logistic Regression (AUC = {roc_auc:.3f})',\n",
    "    line=dict(color='#2ecc71', width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random Classifier (AUC = 0.5)',\n",
    "    line=dict(color='gray', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'ROC Curve - Logistic Regression (AUC = {roc_auc:.3f})',\n",
    "    title_font_size=18,\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    legend=dict(x=0.5, y=0.1)\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f'AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7.3 FEATURE IMPORTANCE (COEFFICIENTS)\n",
    "# ============================================================\n",
    "\n",
    "# Feature names for the 3 scaled inputs [Recency, Frequency, Monetary]\n",
    "feature_names = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    feature_importance,\n",
    "    x='Coefficient',\n",
    "    y='Feature',\n",
    "    orientation='h',\n",
    "    color='Coefficient',\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    title='Logistic Regression Feature Coefficients (Predicting High-Value Customer)',\n",
    "    labels={'Coefficient': 'Coefficient Value', 'Feature': ''},\n",
    "    text='Coefficient'\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    height=400,\n",
    "    title_font_size=18\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print('\\nInterpretation:')\n",
    "for _, row in feature_importance.sort_values('Abs_Coefficient', ascending=False).iterrows():\n",
    "    direction = 'INCREASES' if row['Coefficient'] > 0 else 'DECREASES'\n",
    "    print(f'  {row[\"Feature\"]}: {direction} probability of being High-Value (coeff={row[\"Coefficient\"]:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Revenue Regression Analysis\n",
    "\n",
    "We build a **Linear Regression model** to understand the relationship between customer behavioral features (Recency, Frequency) and their total monetary value. This helps in **revenue forecasting** at the customer level.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. LINEAR REGRESSION - REVENUE PREDICTION\n",
    "# ============================================================\n",
    "\n",
    "# Use log-transform of Monetary for better linearity (revenue is right-skewed)\n",
    "rfm['Log_Monetary'] = np.log1p(rfm['Monetary'])\n",
    "\n",
    "# Features and target\n",
    "X_reg = rfm[['Recency', 'Frequency']]\n",
    "y_reg = rfm['Log_Monetary']\n",
    "\n",
    "# Train-test split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Predictions\n",
    "y_pred_r = lin_reg.predict(X_test_r)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test_r, y_pred_r)\n",
    "mae = mean_absolute_error(y_test_r, y_pred_r)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_r))\n",
    "\n",
    "print('LINEAR REGRESSION RESULTS')\n",
    "print('='*60)\n",
    "print(f'R-squared (R2):             {r2:.4f}')\n",
    "print(f'Mean Absolute Error (MAE):  {mae:.4f}')\n",
    "print(f'Root Mean Sq Error (RMSE):  {rmse:.4f}')\n",
    "print(f'\\nCoefficients:')\n",
    "for feat, coef in zip(X_reg.columns, lin_reg.coef_):\n",
    "    print(f'  {feat}: {coef:.6f}')\n",
    "print(f'  Intercept: {lin_reg.intercept_:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8.1 ACTUAL vs PREDICTED PLOT\n",
    "# ============================================================\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_test_r,\n",
    "    y=y_pred_r,\n",
    "    mode='markers',\n",
    "    marker=dict(color='#3498db', size=5, opacity=0.5),\n",
    "    name='Predictions'\n",
    "))\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(y_test_r.min(), y_pred_r.min())\n",
    "max_val = max(y_test_r.max(), y_pred_r.max())\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[min_val, max_val],\n",
    "    y=[min_val, max_val],\n",
    "    mode='lines',\n",
    "    line=dict(color='#e74c3c', width=2, dash='dash'),\n",
    "    name='Perfect Prediction'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Actual vs Predicted Log(Monetary) | R2 = {r2:.4f}',\n",
    "    title_font_size=18,\n",
    "    xaxis_title='Actual Log(Monetary)',\n",
    "    yaxis_title='Predicted Log(Monetary)',\n",
    "    template='plotly_dark',\n",
    "    height=550\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8.2 RESIDUAL ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "residuals = y_test_r - y_pred_r\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Residuals vs Predicted', 'Residual Distribution')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_pred_r, y=residuals,\n",
    "        mode='markers',\n",
    "        marker=dict(color='#e74c3c', size=4, opacity=0.4),\n",
    "        name='Residuals'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_hline(y=0, line_dash='dash', line_color='white', row=1, col=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=residuals, nbinsx=40,\n",
    "        marker_color='#3498db',\n",
    "        name='Residual Distribution'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Residual Analysis',\n",
    "    title_font_size=18,\n",
    "    template='plotly_dark',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression - Business Interpretation\n",
    "\n",
    "- **Frequency has a strong positive relationship with revenue**: Customers who buy more often spend more overall. This validates the importance of repeat-purchase programs.\n",
    "- **Recency has a negative relationship**: The longer since a customer's last purchase, the lower their predicted revenue. This underscores the need for **re-engagement campaigns** for dormant customers.\n",
    "- The R-squared value indicates the proportion of revenue variance explained by Recency and Frequency alone. Additional features (product preferences, geography, seasonality) would further improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Business Interpretation & Conclusions\n",
    "\n",
    "This section connects our analytical findings to **economic theory** and formulates **actionable business recommendations** for TATA's online retail leadership.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Pareto Principle (80/20 Rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1 PARETO PRINCIPLE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "# Sort customers by revenue descending\n",
    "customer_revenue = rfm[['CustomerID', 'Monetary']].sort_values('Monetary', ascending=False).reset_index(drop=True)\n",
    "customer_revenue['CumulativeRevenue'] = customer_revenue['Monetary'].cumsum()\n",
    "customer_revenue['CumulativeRevenue_Pct'] = customer_revenue['CumulativeRevenue'] / customer_revenue['Monetary'].sum() * 100\n",
    "customer_revenue['Customer_Pct'] = (customer_revenue.index + 1) / len(customer_revenue) * 100\n",
    "\n",
    "# Find what % of customers drive 80% of revenue\n",
    "pct_at_80 = customer_revenue[customer_revenue['CumulativeRevenue_Pct'] >= 80]['Customer_Pct'].iloc[0]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=customer_revenue['Customer_Pct'],\n",
    "    y=customer_revenue['CumulativeRevenue_Pct'],\n",
    "    mode='lines',\n",
    "    line=dict(color='#2ecc71', width=3),\n",
    "    name='Cumulative Revenue',\n",
    "    fill='tozeroy',\n",
    "    fillcolor='rgba(46, 204, 113, 0.15)'\n",
    "))\n",
    "\n",
    "# Perfect equality line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 100], y=[0, 100],\n",
    "    mode='lines',\n",
    "    line=dict(color='gray', dash='dash'),\n",
    "    name='Perfect Equality'\n",
    "))\n",
    "\n",
    "# 80% revenue line\n",
    "fig.add_hline(y=80, line_dash='dot', line_color='#e74c3c',\n",
    "              annotation_text='80% Revenue', annotation_position='top left')\n",
    "fig.add_vline(x=pct_at_80, line_dash='dot', line_color='#e74c3c',\n",
    "              annotation_text=f'{pct_at_80:.1f}% Customers', annotation_position='top right')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Pareto Analysis: {pct_at_80:.1f}% of Customers Drive 80% of Revenue',\n",
    "    title_font_size=18,\n",
    "    xaxis_title='% of Customers (ranked by revenue)',\n",
    "    yaxis_title='% of Cumulative Revenue',\n",
    "    template='plotly_dark',\n",
    "    height=550\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f'PARETO PRINCIPLE VALIDATION:')\n",
    "print(f'  {pct_at_80:.1f}% of customers generate 80% of total revenue.')\n",
    "print(f'  This closely follows the classic 80/20 Pareto distribution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Demand-Supply Analysis: Peak Demand Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2 DEMAND HEATMAP: DAY OF WEEK vs HOUR\n",
    "# ============================================================\n",
    "\n",
    "demand_heatmap = df.groupby(['DayName', 'Hour'])['TotalRevenue'].sum().reset_index()\n",
    "demand_pivot = demand_heatmap.pivot_table(index='DayName', columns='Hour', values='TotalRevenue', fill_value=0)\n",
    "\n",
    "# Reorder days\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "demand_pivot = demand_pivot.reindex(day_order)\n",
    "\n",
    "fig = px.imshow(\n",
    "    demand_pivot,\n",
    "    color_continuous_scale='YlOrRd',\n",
    "    title='Revenue Heatmap: Day of Week vs Hour of Day',\n",
    "    labels=dict(x='Hour of Day', y='Day of Week', color='Revenue (GBP)'),\n",
    "    aspect='auto'\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    height=500,\n",
    "    title_font_size=18\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9.3 Comprehensive Business Conclusions\n",
    "\n",
    "#### Economic Concepts Applied\n",
    "\n",
    "| Economic Concept | Finding | Implication |\n",
    "|-----------------|---------|-------------|\n",
    "| **Pareto Principle** | A small fraction of customers generates the vast majority of revenue | Focus retention strategies on the top 20% high-value customers |\n",
    "| **Demand Elasticity** | Revenue peaks during specific months (Q4), weekdays, and mid-day hours (10 AM - 2 PM) | Align marketing campaigns, staffing, and inventory with peak demand windows |\n",
    "| **Market Concentration** | UK dominates revenue; Netherlands, EIRE, Germany are top international markets | Pursue focused international expansion in high-performing European markets |\n",
    "| **Customer Lifetime Value** | Champions and Loyal Customer segments drive disproportionate revenue | Invest in loyalty programs and personalized offers for these segments |\n",
    "| **Price Sensitivity** | Revenue is driven more by quantity than unit price (correlation analysis) | Volume-based promotions (bundles, multi-buy) may be more effective than discounting |\n",
    "| **Churn Risk** | \"At Risk\" and \"Lost\" segments represent customers who were once active but have disengaged | Implement automated re-engagement campaigns (email, promotions) for these segments |\n",
    "\n",
    "---\n",
    "\n",
    "#### Actionable Business Recommendations\n",
    "\n",
    "1. **Implement a Tiered Loyalty Program:** Based on RFM segments, offer differentiated benefits -- exclusive early access for Champions, milestone rewards for Loyal Customers, and win-back offers for At-Risk customers.\n",
    "\n",
    "2. **Optimize Marketing Spend by Time:** Concentrate digital ad spending and email campaigns during peak engagement windows (weekday mornings, 10 AM - 2 PM). Reduce spend during off-peak hours and weekends (especially Saturday).\n",
    "\n",
    "3. **Expand Strategically in Europe:** The Netherlands, Ireland, Germany, and France represent the strongest international markets. Invest in localized websites, local payment methods, and regional warehousing to reduce delivery times.\n",
    "\n",
    "4. **Deploy Predictive Churn Models:** Use the Logistic Regression model as a foundation for identifying customers likely to churn. Trigger automated re-engagement sequences (discount codes, product recommendations) when a customer's predicted value drops.\n",
    "\n",
    "5. **Seasonal Inventory Planning:** Q4 (September - November) shows the strongest revenue growth, likely driven by holiday purchasing. Pre-position inventory for top-selling products 6-8 weeks before the peak to avoid stockouts.\n",
    "\n",
    "6. **Bundle and Cross-Sell Strategy:** Top-selling products should be combined into curated bundles. Use association analysis on customer purchase baskets to identify natural product pairings for cross-sell recommendations.\n",
    "\n",
    "7. **Personalized Pricing & Promotions:** Rather than blanket discounts, use cluster-based targeting. High-frequency, low-monetary customers may respond to value bundles, while infrequent high-spenders may respond to premium product launches.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary of AI Techniques and Business Value\n",
    "\n",
    "| Technique | Type | Business Value |\n",
    "|-----------|------|----------------|\n",
    "| **RFM Analysis** | Descriptive Analytics | Segments customers into actionable groups for targeted marketing |\n",
    "| **K-Means Clustering** | Unsupervised ML | Discovers natural customer groupings without manual rules; validates and enriches RFM segments |\n",
    "| **Logistic Regression** | Supervised ML (Classification) | Predicts high-value customers for proactive retention; identifies which features signal value |\n",
    "| **Linear Regression** | Supervised ML (Regression) | Quantifies the relationship between customer behavior and revenue; supports forecasting |\n",
    "| **Exploratory Data Analysis** | Descriptive Analytics | Reveals temporal, geographic, and product-level revenue patterns for operational planning |\n",
    "\n",
    "> These AI-driven insights enable TATA's retail division to move from **intuition-based** to **evidence-based** decision-making, directly improving revenue efficiency and customer satisfaction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Models & Outputs\n",
    "\n",
    "We persist all trained models and key outputs to enable future deployment, scoring, and analysis without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. SAVE MODELS & OUTPUTS\n",
    "# ============================================================\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save K-Means model\n",
    "joblib.dump(kmeans, 'models/kmeans_model.pkl')\n",
    "print('Saved: models/kmeans_model.pkl')\n",
    "\n",
    "# Save Logistic Regression model\n",
    "joblib.dump(log_reg, 'models/logistic_regression_model.pkl')\n",
    "print('Saved: models/logistic_regression_model.pkl')\n",
    "\n",
    "# Save Linear Regression model\n",
    "joblib.dump(lin_reg, 'models/linear_regression_model.pkl')\n",
    "print('Saved: models/linear_regression_model.pkl')\n",
    "\n",
    "# Save StandardScaler\n",
    "joblib.dump(scaler, 'models/standard_scaler.pkl')\n",
    "print('Saved: models/standard_scaler.pkl')\n",
    "\n",
    "# Save RFM DataFrame\n",
    "rfm.to_csv('models/rfm_analysis.csv', index=False)\n",
    "print('Saved: models/rfm_analysis.csv')\n",
    "\n",
    "print('\\nAll models and outputs saved successfully.')\n",
    "print(f'Files in models/ directory: {os.listdir(\"models\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### End of Analysis\n",
    "\n",
    "**Project:** TATA Online Retail Store - Revenue Drivers Analysis  \n",
    "**Group 11:** Payal Kunwar, Gaurav Kulkarni, Anshuman Atrey, Abdullah Haque, Shlok Vijay Kadam  \n",
    "**Course:** Business Studies with Applied AI\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
